{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88c1bbe8-cd17-4050-86b7-8498b253cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.quantization import get_default_qconfig, prepare, convert, fuse_modules\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "# from Model.Schema import resModel\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8577cb8-c1bf-40d8-bbd7-21287400b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.precision = torchmetrics.Precision(task=\"multiclass\", average=\"macro\", num_classes=num_classes)\n",
    "        self.recall = torchmetrics.Recall(task=\"multiclass\", average=\"macro\", num_classes=num_classes)\n",
    "        self.f1score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)  # Quantize the input\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)  # Dequantize the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f84911eb-c915-41a7-9840-6ec17ed4dbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=70, bias=True)\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (accuracy): MulticlassAccuracy()\n",
       "  (precision): MulticlassPrecision()\n",
       "  (recall): MulticlassRecall()\n",
       "  (f1score): MulticlassF1Score()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 70\n",
    "model = resModel(num_classes)\n",
    "model.load_state_dict(torch.load('Weights/model_13.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "081a9be3-dfa9-49cc-9c32-7788a44691a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend = \"fbgemm\"\n",
    "# model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "# torch.backends.quantized.engine = backend\n",
    "# model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "# model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1feb2ea3-fc4f-4582-b8aa-a564e1aa0532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating: 100%|█████████████████████████████████████████████████████████████████████| 25/25 [00:38<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# modules_to_fuse = [\n",
    "#     [\"model.layer1.0.conv1\", \"model.layer1.0.bn1\", \"model.layer1.0.relu\"],\n",
    "#     [\"model.layer1.0.conv2\", \"model.layer1.0.bn2\"],\n",
    "#     [\"model.layer1.1.conv1\", \"model.layer1.1.bn1\", \"model.layer1.1.relu\"],\n",
    "#     [\"model.layer1.1.conv2\", \"model.layer1.1.bn2\"],\n",
    "\n",
    "#     [\"model.layer2.0.conv1\", \"model.layer2.0.bn1\", \"model.layer2.0.relu\"],\n",
    "#     [\"model.layer2.0.conv2\", \"model.layer2.0.bn2\"],\n",
    "#     [\"model.layer2.0.downsample.0\", \"model.layer2.0.downsample.1\"],\n",
    "#     [\"model.layer2.1.conv1\", \"model.layer2.1.bn1\", \"model.layer2.1.relu\"],\n",
    "#     [\"model.layer2.1.conv2\", \"model.layer2.1.bn2\"],\n",
    "\n",
    "#     [\"model.layer3.0.conv1\", \"model.layer3.0.bn1\", \"model.layer3.0.relu\"],\n",
    "#     [\"model.layer3.0.conv2\", \"model.layer3.0.bn2\"],\n",
    "#     [\"model.layer3.0.downsample.0\", \"model.layer3.0.downsample.1\"],\n",
    "#     [\"model.layer3.1.conv1\", \"model.layer3.1.bn1\", \"model.layer3.1.relu\"],\n",
    "#     [\"model.layer3.1.conv2\", \"model.layer3.1.bn2\"],\n",
    "\n",
    "#     [\"model.layer4.0.conv1\", \"model.layer4.0.bn1\", \"model.layer4.0.relu\"],\n",
    "#     [\"model.layer4.0.conv2\", \"model.layer4.0.bn2\"],\n",
    "#     [\"model.layer4.0.downsample.0\", \"model.layer4.0.downsample.1\"],\n",
    "#     [\"model.layer4.1.conv1\", \"model.layer4.1.bn1\", \"model.layer4.1.relu\"],\n",
    "#     [\"model.layer4.1.conv2\", \"model.layer4.1.bn2\"],\n",
    "# ]\n",
    "\n",
    "# for module_list in modules_to_fuse:\n",
    "#     fuse_modules(model, module_list, inplace=True)\n",
    "\n",
    "model.qconfig = get_default_qconfig('fbgemm')\n",
    "\n",
    "model_prepared = prepare(model, inplace=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root='dataset/train', transform=transform)\n",
    "\n",
    "num_calibration = int(0.1 * len(full_dataset))\n",
    "num_rest = len(full_dataset) - num_calibration\n",
    "calibration_dataset, _ = random_split(full_dataset, [num_calibration, num_rest])\n",
    "\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model_prepared.eval()\n",
    "with torch.no_grad():\n",
    "    for data, _ in tqdm(calibration_loader, desc=\"Calibrating\", leave=True):\n",
    "        data = data.to('cpu')\n",
    "        model_prepared(data)\n",
    "\n",
    "        \n",
    "model_prepared.to('cpu')\n",
    "\n",
    "model_quantized = convert(model_prepared, inplace=False)\n",
    "\n",
    "torch.save(model_quantized.state_dict(), 'Weights/model_quantized_13.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6ba39e5-d76c-4864-b5fc-4643e2aeccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# image_path = 'dataset/valid/American  Spaniel/07.jpg'\n",
    "# image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# model_quantized.eval()\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "# model_quantized.to(device)\n",
    "# image = image.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model_quantized(image)\n",
    "#     prediction = torch.argmax(output, dim=1)\n",
    "\n",
    "# print(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4566ec4c-db45-48c6-bdbc-60aeed5ae0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): QuantizedConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.11172884702682495, zero_point=51, padding=(3, 3), bias=False)\n",
       "    (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.19457167387008667, zero_point=79, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.06815629452466965, zero_point=71, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.16469429433345795, zero_point=69, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.05816025659441948, zero_point=65, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.14372628927230835, zero_point=70, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04535432532429695, zero_point=74, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.042752474546432495, zero_point=63, bias=False)\n",
       "          (1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.09039149433374405, zero_point=65, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04443883150815964, zero_point=70, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.08741097152233124, zero_point=67, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0671195238828659, zero_point=69, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.0211283378303051, zero_point=68, bias=False)\n",
       "          (1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.12083142250776291, zero_point=63, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0667150616645813, zero_point=72, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.08619529753923416, zero_point=70, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.04569511488080025, zero_point=69, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.021378569304943085, zero_point=64, bias=False)\n",
       "          (1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.14192654192447662, zero_point=72, padding=(1, 1), bias=False)\n",
       "        (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.09502904862165451, zero_point=50, padding=(1, 1), bias=False)\n",
       "        (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): QuantizedLinear(in_features=512, out_features=70, scale=0.48215940594673157, zero_point=77, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (accuracy): MulticlassAccuracy()\n",
       "  (precision): MulticlassPrecision()\n",
       "  (recall): MulticlassRecall()\n",
       "  (f1score): MulticlassF1Score()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_fp32 = resModel(num_classes=70)\n",
    "# model_fp32.load_state_dict(torch.load(\"Weights/model_13.pth\"))\n",
    "\n",
    "# model_fp32.train()\n",
    "\n",
    "# model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# model_fp32_prepared = torch.quantization.prepare_qat(model_fp32)\n",
    "\n",
    "# model_quantized = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# model_quantized.load_state_dict(torch.load('Weights/model_quantized_13.pth'))\n",
    "\n",
    "model_quantized.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dd3b96c-01c9-4010-a791-81d71ace0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|                                                                                  | 0/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nMkldnnCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterMkldnnCPU.cpp:515 [kernel]\nSparseCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterFunctionalization_0.cpp:21977 [kernel]\nNamed: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\ADInplaceOrViewType_0.cpp:4832 [kernel]\nAutogradOther: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradHIP: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradXLA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradMPS: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradIPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradXPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradHPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradVE: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradLazy: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradMTIA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradMeta: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradNestedTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nTracer: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:17346 [kernel]\nAutocastCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m device\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# model_quantized.to(device)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m test_quantized_model(model_quantized, test_loader, device)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# wandb.log({\"Test Accuracy\": accuracy})\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy of the quantized model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 33\u001b[0m, in \u001b[0;36mtest_quantized_model\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 33\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     34\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m, in \u001b[0;36mresModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)  \u001b[38;5;66;03m# Quantize the input\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(x)  \u001b[38;5;66;03m# Dequantize the output\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torchvision\\models\\resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_homework\\Lib\\site-packages\\torchvision\\models\\resnet.py:102\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m--> 102\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[0;32m    103\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nMkldnnCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterMkldnnCPU.cpp:515 [kernel]\nSparseCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterFunctionalization_0.cpp:21977 [kernel]\nNamed: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\ADInplaceOrViewType_0.cpp:4832 [kernel]\nAutogradOther: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradHIP: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradXLA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradMPS: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradIPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradXPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradHPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradVE: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradLazy: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradMTIA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradMeta: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nAutogradNestedTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:17434 [autograd kernel]\nTracer: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:17346 [kernel]\nAutocastCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from Model.Schema import resModel\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"EML\",\n",
    "#     config={\n",
    "#         \"architecture\": \"PretrainedResNet18\",\n",
    "#         \"dataset\": \"70 Dog Breeds\",\n",
    "#         \"quantization\": \"Static\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(root='dataset/test', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def test_quantized_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Testing\", leave=True):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    test_accuracy = 100 * correct / total\n",
    "    return test_accuracy\n",
    "\n",
    "device= torch.device('cpu')\n",
    "# model_quantized.to(device)\n",
    "\n",
    "accuracy = test_quantized_model(model_quantized, test_loader, device)\n",
    "# wandb.log({\"Test Accuracy\": accuracy})\n",
    "print(f\"Test Accuracy of the quantized model: {accuracy}%\")\n",
    "\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1802f4-ed23-4e50-9a71-5b121e38bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cpu')\n",
    "# model_quantized.to(device)\n",
    "\n",
    "accuracy = test_quantized_model(model_quantized, test_loader, device)\n",
    "# wandb.log({\"Test Accuracy\": accuracy})\n",
    "print(f\"Test Accuracy of the quantized model: {accuracy}%\")\n",
    "\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8fe473-5937-4be8-8c21-d5b2aa5a8978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
